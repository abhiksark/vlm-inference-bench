# configs/benchmark_vllm_thinking.yaml
# Benchmark configuration for vLLM backend with thinking mode

backend:
  type: "vllm"
  base_url: "http://localhost:8001"
  model: "Qwen/Qwen3-VL-4B-Instruct"
  max_new_tokens: 512
  timeout: 180

data:
  video_dir: "video"
  sample_size: 5
  frame_sampling: "uniform"
  frames_per_video: 4
  target_fps: 1

benchmark:
  warmup_runs: 1
  num_runs: 5
  batch_sizes: [1]
  measure_memory: true
  measure_gpu_util: true
  save_individual_results: true
  prompt_prefix: "/think "

output:
  results_dir: "results"
  save_format: ["json", "csv"]
