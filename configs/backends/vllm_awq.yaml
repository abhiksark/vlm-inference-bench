# configs/backends/vllm_awq.yaml
# vLLM with AWQ quantization for lower memory usage

backend:
  type: vllm
  base_url: "http://localhost:28001"
  model: "Qwen/Qwen3-VL-4B-Instruct-AWQ"
  dtype: float16
  max_new_tokens: 256
  timeout: 120.0
  options:
    quantization: awq
    gpu_memory_utilization: 0.85
    tensor_parallel_size: 1

# Server start command:
# vllm serve Qwen/Qwen3-VL-4B-Instruct-AWQ \
#   --quantization awq \
#   --dtype float16 \
#   --gpu-memory-utilization 0.85 \
#   --port 8001
