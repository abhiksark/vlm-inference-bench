# configs/backends/vllm_kvcache.yaml
# vLLM with KV cache optimizations (prefix caching + chunked prefill)

backend:
  type: vllm
  base_url: "http://localhost:8002"
  model: "Qwen/Qwen3-VL-4B-Instruct"
  dtype: bfloat16
  max_new_tokens: 256
  timeout: 120.0
  options:
    attention: flash_attn_2
    gpu_memory_utilization: 0.90
    enable_prefix_caching: true
    enable_chunked_prefill: true
    tensor_parallel_size: 1

# Server start command:
# vllm serve Qwen/Qwen3-VL-4B-Instruct \
#   --dtype bfloat16 \
#   --gpu-memory-utilization 0.90 \
#   --enable-prefix-caching \
#   --enable-chunked-prefill \
#   --port 8002
