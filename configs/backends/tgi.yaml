# configs/backends/tgi.yaml
# HuggingFace Text Generation Inference backend

backend:
  type: tgi
  base_url: "http://localhost:8003"
  model: "Qwen/Qwen3-VL-4B-Instruct"
  dtype: bfloat16
  max_new_tokens: 256
  timeout: 120.0
  options:
    max_total_tokens: 4096
    max_input_length: 2048

# Server start command:
# docker run --gpus all -p 8003:80 \
#   -v $PWD/data:/data \
#   ghcr.io/huggingface/text-generation-inference:latest \
#   --model-id Qwen/Qwen3-VL-4B-Instruct \
#   --dtype bfloat16 \
#   --max-total-tokens 4096 \
#   --max-input-length 2048
